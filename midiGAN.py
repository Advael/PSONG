# -*- coding: utf-8 -*-
"""midiGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bs4w_ig_k_uwFhcZY0YhRJrSDH4y5cks
"""

## model.py 
# Will define GAN constructs 

import tensorflow as tf
import numpy as np

def make_cell(cell_shape, dropout_keep_prob=1.0, base_cell=tf.contrib.rnn.BasicLSTMCell, state_is_tuple=True, reuse=False):
    cell = base_cell(num_units, state_is_tuple=state_is_tuple, reuse=reuse)
    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout_keep_prob)
    return cell
  
def data_type():
    return tf.float16 if FLAGS.float16 else tf.float32
  
def linear(inp, output_dim, scope=None, stddev=1.0, reuse_scope=False):

    norm = tf.random_normal_initializer(stddev=stddev, dtype=data_type())
    const = tf.constant_initializer(0.0, dtype=data_type())
    with tf.variable_scope(scope or 'linear') as scope:
        scope.set_regularizer(tf.contrib.layers.l2_regularizer(scale=FLAGS.reg_scale))
        if reuse_scope:
            scope.reuse_variables()
        #print('inp.get_shape(): {}'.format(inp.get_shape()))
        w = tf.get_variable('w', [inp.get_shape()[1], output_dim], initializer=norm, dtype=data_type())
        b = tf.get_variable('b', [output_dim], initializer=const, dtype=data_type())
    to_print = tf.matmul(inp, w) + b
  return to_print  


class midiGAN: 
    def __init__(self, sess, cellType, dims, G_layers, D_layers, preTraining = False, threshold = 0.9, batch_size = 1, pixels):
      #Set hyperparameters, layer sizes, input/output dims, etc.
        
        self.batch_size = batch_size
        self.seesion = sess
        self.cellType = cellType
        self.glayers = G_layers
        self.dlayers = D_layers
        self.pre_train = preTraining
        self.dimensions = dims
        self.threshold = threshold
        
        # Memory build up for discriminator
        # index access each time step
        self.lstm_layers_discriminator = list()
        self.time_step_states_discriminator = list()
        self.confidence_scores = list()
        
        
        # generator setup 
        self.init_state = None
        self.lstm_layers_generator = list()
        self.temp_step_states_generator = list()
        self.generated_notes = list()
        
        
        # Pong setup 
        self.pixel_size = pixels
        
        
        # self.build_model()
        
    def build_model(self):
        #The actual part where you build the model
        
        
        
     
    def discriminate(self, note):
      
        confidence = 0.0
        cell = make_cell(self.dlayers)
        self.lstm_layers_discriminator.append(cell)
        
        # handling init/later states
        if len(self.time_step_states_discriminator) < 1:
            state = cell.zero_state(self.batch_size)
        else:
            state = self.time_step_states_discriminator[-1]
        
        # output, state = tf.nn.rnn(cell, note)
        output, state = cell(note, state)
        self.time_step_states.append(state)
        confidence = tf.sigmoid(output)
        self.Printer(confidence, "Confidence")
        
        # recording the confidence scores
        self.confidence_scores.append(confidence)
        return confidence
    
    def Printer(self, to_print, message):
        temp = tf.Print(to_print, [to_print], message + " = ", summarize = 10)
        print(temp)
  
  
    accept = lambda self, note : (self.discriminate(note) > self.threshold)
    
    def generateNoise(self, size, minimum = 0.0, maximum = 1.0):
        # size will ideally be a tuple 
        return tf.random_uniform(shape = size, minval = minimum, maxval = maximum)
    
    def generateNote(self, inputs, noise = False):
        # pixels and noise are the input 
        if noise:
            inputs = tf.concat([inputs, noise])
            
        # Fully connected layer
        inputs = tf.nn.relu(linear(inputs, self.glayers, scope = 'input_layer'))
        cell = make_cell(self.glayers)
        
        # LSTM 
        if(len(self.time_step_states_generator) < 1):
            state = cell.zero_state(self.batch_size)
        else:
            state = self.time_step_states_generator[-1]
            
        self.lstm_layers_generator.append(cell)
        intermediate_output, state = cell(inputs, state)
        self.time_step_states_generator.append(state)
        
        # Note generation
        note = linear(output, self.pixel_size)
        
        # Saving notes to model
        self.generated_notes.append(note)
        return note
      
    @property
    def sample_generated_notes(self):
        return self.generated_notes
    
    def disc_backward(self, errors):
        #backprop discriminator error
        
        
    def gen_backward(self, errors):
        #backprop error over the generator
        
        
    def reset()
        states = []
        D_cells = []

# Pong agent

# top level script init-ing model and pong
import tensorflow as tf
import gym
import model
import argparse
import random
# from model import midiGAN, pongAgent

nb_actions = 3
parser = argparse.ArgumentParser()
parser.add_argument('--iter', type=int, default=1000)
args = parser.parse_args()
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
env = gym.make('Pong-v0')

with tf.Session(config=config) as sess:
    mg = midiGAN(sess, dims, checkpoint_dir='checkpoint')
    observation = env.reset()
    done = False
    while iterations < nIter:
        steps = 0
        while done is False:
            o = preprocess(observation)
            note = mg.generateNote(o)
            if mg.accept(note):
                action = agent.act(note)
            else:
                action = agent.act(random.choice(range(nb_actions)))
            observation, reward, done, _ = env.step(action)
            agent.save_reward(reward)
            print("Timestep")
            if reward is not 0:
                print("Reward {} at step {}".format(reward, steps))
                errors = agent.reinforce()
                mg.gen_backward(errors)
                mg.disc_backward()
            iterations += 1
        steps += 1
        agent.reset()
        env.reset()
        done = False
        print("Done at step {}".format(steps))
    sess.close()
